{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.fftpack import dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0\n",
    "\n",
    "1. Write a function that will compute a STFT of a single channel of audio.\n",
    "2. Read in any audio one of the audio files provided, plot your results using the provided `plotSTFT()` function. (If you did this part correctly, you should be getting a plot very similar to what you obtain from `specgram()` of last week's lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stft(x, N = 256, M = 128):\n",
    "    ## this function should take in a signal x, \n",
    "    ## and compute a N-point STFT with step size M\n",
    "    ## with a Hamming window applied to it\n",
    "\n",
    "    n = len(x)\n",
    "    L = N-M\n",
    "    K = (n-N)/M + 1\n",
    "    N2 = N/2 \n",
    "    S = np.zeros([N2+1,K+1])\n",
    "    \n",
    "    ## Your Code here\n",
    "    start = 0\n",
    "    while start + M < n:\n",
    "        signal = x[start:start + N] * np.hamming(start +N - start)\n",
    "\n",
    "        start += M\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotSTFT(S):\n",
    "    N = S.shape[0]\n",
    "    tt = np.arange(S.shape[1])\n",
    "    freq = np.hstack([np.arange(0., N)/N, 1])\n",
    "    plt.pcolormesh(tt, freq, 10 * np.log10(S[:,:-1]))  # plot your spectrogram\n",
    "    plt.axis('tight')\n",
    "    plt.ylabel('frequency (normalized)')\n",
    "    plt.xlabel('time (in samples)')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-424 -460 -501 ...,  154  174  222]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5ecb6306a1a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mFs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwavfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/chopin.wav'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplotSTFT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stft' is not defined"
     ]
    }
   ],
   "source": [
    "Fs, s = wavfile.read('data/chopin.wav')\n",
    "S = stft(s[:,0])\n",
    "\n",
    "plotSTFT(S) \n",
    "plt.title('My Spectrogram of Audio Signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel Frequency Cepstrum\n",
    "\n",
    "The Mel Frequency Ceptrum is a human auditory response insipred audio feature that is being used succesfully in many audio processing applications. We will need some math tools from scipy to implement this audio feature extractor.\n",
    "\n",
    "The Mel Frequency Cepstral Coefficients (MFCC) are computed as follows:\n",
    "\n",
    "1. Compute the Fourier Transform of the signal\n",
    "2. Map power spectrum to the mel scale, using overlapping triangular windows (filter banks)\n",
    "3. Take log of the power spectrum in the mel 'frequency'\n",
    "4. Compute the Discrete Cosine Transform of the spectrum computed above, as though it were a signal\n",
    "\n",
    "The amplitudes obtained are the MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting from frequency to mel qerfuency\n",
    "\n",
    "First, two important formulas:\n",
    "\n",
    "Formula to convert frequency $f$ Hertz to $m$ Mel:\n",
    "\n",
    "$$m = 2595 * log_{10}(1+\\frac{f}{700})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "1. Derive the formula to convert from Mel back to Hertz\n",
    "2. Implement these 2 conversions as functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hz2mel(h):\n",
    "    ## This function should output the Mel quefrency given some input frequency\n",
    "    ## h is a 1-d array of input frequencies in hertz, \n",
    "    ## Your code here\n",
    "    return 0\n",
    "\n",
    "def mel2hz(m):\n",
    "    ## This function should output the frequency in Hertz given some input quefrency\n",
    "    ## m is a 1-d array of input quefencies in Mel, \n",
    "    ##Your code here\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel-spaced filter banks\n",
    "\n",
    "The Mel-spaced filterbank is the set of triangular window filter which corresponds to human ear hearing perception. The following code is to use to generate **nfilt** filters using the end points supplied by binPoints vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter bank generation\n",
    "def filterBank(nfilt, NFFT, binPoints):\n",
    "    fbank = np.zeros([nfilt,NFFT/2+1])\n",
    "    for j in xrange(0,nfilt):\n",
    "        for i in xrange(int(binPoints[j]),int(binPoints[j+1])):\n",
    "            fbank[j,i] = (i - binPoints[j])/(binPoints[j+1]-binPoints[j]) #rising edge of the triangle\n",
    "        for i in xrange(int(binPoints[j+1]),int(binPoints[j+2])):\n",
    "            fbank[j,i] = (binPoints[j+2]-i)/(binPoints[j+2]-binPoints[j+1]) #falling edge of the triangle\n",
    "    return fbank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at how the Mel-spaced filter banks are applied to the power spectrum of an audio signal. The following code generates the filter banks given the sampling frequency, number of filters, and length of the FFT.\n",
    "\n",
    "(You will need to finish the code for previous section for this part to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Fs, x = wavfile.read('data/WakeMeUp.wav')     # x stores the input audio signal, Fs is the sampling frequency\n",
    "signal = x[:,0]\n",
    "\n",
    "highfreq = Fs/2       # Nyquist rate\n",
    "lowfreq = 0           # lower end of audio spectrum\n",
    "\n",
    "lowmel = hz2mel(lowfreq)     # convert to Mel scales\n",
    "highmel = hz2mel(highfreq)\n",
    "\n",
    "NFFT = 512            # number of FFT points\n",
    "nfilt = 20            # number of Mel spaced filter banks\n",
    "\n",
    "# convert the frequency into Mel points\n",
    "melPoints = np.linspace(lowmel, highmel, nfilt + 2)\n",
    "binPoints = np.floor(mel2hz(melPoints)/Fs*(NFFT+1))\n",
    "\n",
    "fbank = filterBank(nfilt, NFFT, binPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a visualization of the 20 filter banks in the frequency domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq = np.arange(257)*(Fs/2.)/257    # generate frequencies for plot\n",
    "plt.figure()\n",
    "for i in np.arange(nfilt):\n",
    "    plt.plot(freq, fbank[i,:])\n",
    "plt.title('Filter Bank Responses')\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('frequency')\n",
    "ax.set_ylabel('amplitude')\n",
    "ax.set_xlim([0, 22050]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the power spectrum of a window of the audio signal. The plot is of what would be one column of the spectrogram as created in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = 10000\n",
    "N=300\n",
    "w = np.hamming(N)      # generate a Hamming window of length 300\n",
    "s = signal[T:T+NFFT]\n",
    "f =  np.square((1.0/NFFT) *np.absolute(np.fft.rfft(s,)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(freq, f)\n",
    "plt.title('Power Spectrum (PS) of Signal')\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('frequency')\n",
    "ax.set_ylabel('magnitude')\n",
    "ax.set_xlim([0, 22050])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we see the result of applying the 12th filter bank to the window of the audio signal. To apply the filter, the filter and the PS of the signal are simply multiplied together. The plot below is the product of the filter and the PS of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_f = fbank*np.tile(f, (nfilt, 1))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(freq, filtered_f[11,:])\n",
    "plt.title('PS of Signal Filtered by 12th Filter Bank')\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('frequency')\n",
    "ax.set_ylabel('magnitude')\n",
    "ax.set_xlim([0, 22050])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what we will need for features are actually are the inner products of the PS with the filters, and not the multiplication, thus creating *coefficients*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fbank_coefs = np.dot(fbank,f)\n",
    "print fbank_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the MFCC's\n",
    "\n",
    "The MFCC's are the DCT of the log of the filter bank coefficients. For the window of the input audio signal, the coefficients are computed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "melMapped = np.log(fbank_coefs) # take the log of the mel-spaced mapping signal\n",
    "mfcc = dct(melMapped.T, type = 2, axis = 0, norm = 'ortho')# take the Discrete Cosine Transform\n",
    "print mfcc\n",
    "\n",
    "plt.plot(mfcc)\n",
    "plt.title('MFCC of window of \"WakeMeUp.wav\"')\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('frequency bin')\n",
    "ax.set_ylabel('value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the coefficients of a window of 'further.wav' to a window of a different file, 'chopin.wav,' of a different genre of music with different frequency content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Fs2, x2 = wavfile.read('data/chopin.wav')     # x stores the input audio signal, Fs is the sampling frequency\n",
    "signal2 = x2[:,0]\n",
    "\n",
    "T = 10000\n",
    "w2 = np.hamming(N)      # generate a Hamming window of length 300\n",
    "s2 = signal2[T:T+NFFT]\n",
    "f2 =  np.square((1.0/NFFT) *np.absolute(np.fft.rfft(s2,)))\n",
    "\n",
    "fbank_coefs2 = np.dot(fbank,f2)\n",
    "\n",
    "melMapped2 = np.log(fbank_coefs2) # take the log of the mel-spaced mapping signal\n",
    "mfcc2 = dct(melMapped2.T, type = 2, axis = 0, norm = 'ortho')# take the Discrete Cosine Transform\n",
    "\n",
    "plt.plot(mfcc2)\n",
    "plt.title('MFCC of window of \"chopin.wav\"')\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('frequency bin')\n",
    "ax.set_ylabel('value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "With the information and code examples provided above, you should now be able to implement a function to compute the MFCC of a given signal. (Hint, you will need the functions that you have defined so far, and other crucial code components are already provided above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMFCC(Fs, data):\n",
    "    #Your code here\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Identification\n",
    "\n",
    "In this exercise, we will utilize the MFCC feature to recognize the identity of a speaker. You should put the two folders that were provided, train and test, into the same folder of your ipython notebook. There are 8 training data files and 8 testing data files, s1.wav to s8.wav. The training and testing data are already correctly labeled, i.e, s1.wav in the training folder matches with s1.wav in the testing folder (same person speaking). Nearest neighbor will be used to label the testing data.\n",
    "\n",
    "Each audio file in this dataset is mono-channel, thus you do not need to split the files into two channels as in the previous part. However the files are of different length, which requires special attention to allocate the training dataset. For each audio file in the train fodler, we create its MFCC matrix, where each row corresponds to 12 MFCC's of one frame. If your signal has 125 frames, for instance, the size of MFCC matrix will be 125 by 12. The MFCC matrix of all audio files are stacked into one matrix called the **codebook**. At the same time, we also store the labels of the training data. \n",
    "\n",
    "## Training\n",
    "\n",
    "We first calculate the total number of frames of our whole dataset. This is used to initialize the codebook matrix and the label vector. Let's say you have only 2 audio files. The first file has 10 frames, and the second has 20 frames. Then the size of your **codebook** is 30 by 12. \n",
    "\n",
    "The length of your **labels** vector will be 30, in which the first 10 elements are *1* and the last 20 elements are *2*. For this exercise, we will choose the length of each frame to be 512 and the step size is one third of the frame length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mfcclab\n",
    "# find the number of frames in advance\n",
    "# this is to reduce dynamic array allocation \n",
    "# which slows down our code\n",
    "N = 512\n",
    "M = 512/3\n",
    "sumNumFrames = mfcclab.get_num_frames(\"train\", N, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the range of frequencies for our filter banks in Hertz, *lowfreq* and *highfreq* (from 0 Hz to the Nyquist rate). To create the endpoints for our filter banks, we transform this range into mel-scale range. The *for* loop loads all the training data in the **train** folder, computes the MFCC's, and appends them to the **codebook**. Since we know the labels of our data, this is considered an example of *supervised learning*, as opposed to an *unsupervised learning* paradigm, like K-means clustering, where the labels are unknown and must be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "highfreq = Fs/2 # Nyquist rate\n",
    "lowfreq = 0 # lower end of audio spectrum\n",
    "\n",
    "lowmel = hz2mel(lowfreq) # convert to Mel scales\n",
    "highmel = hz2mel(highfreq)\n",
    "\n",
    "NFFT = 512 # number of FFT points\n",
    "\n",
    "nfilt = 20 # number of Mel spaced filter banks\n",
    "# convert the frequency into Mel points\n",
    "#melPoints = np.linspace(lowmel, highmel, nfilt + 2)\n",
    "#binPoints = np.floor(mel2hz(melPoints)/Fs*(NFFT+1))\n",
    "\n",
    "numKeptCoeff = 12 # we only keep the lower 12 coefficients since the higher frequencies do not help speaker identification\n",
    "#fbank = filterBank(nfilt, NFFT, binPoints) # create the Mel-spaced filter bank\n",
    "codeBook = np.empty((sumNumFrames, numKeptCoeff-1)) #preallocate the training data\n",
    "labels = np.zeros((sumNumFrames,)) # and the labels\n",
    "N = 512 # lenghth of a frame\n",
    "M = 512/3 # step size\n",
    "currentIdx = 0\n",
    "for i in range(1,9): # we have 8 signals\n",
    "    filename = \"train/s%d.wav\" % i\n",
    "    fs, data = wavfile.read(filename)\n",
    "    mfcc = calcMFCC(fs, data, 20)[:,1:numKeptCoeff]\n",
    "    codeBook[currentIdx:currentIdx + mfcc.shape[0],:] = mfcc# assign this training data to codebook\n",
    "    labels[currentIdx:currentIdx + mfcc.shape[0]] = i # and label them\n",
    "    currentIdx = currentIdx + mfcc.shape[0] # move to next block in codeBook and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the each codeword in the codebook is of dimension 11, it is not possible to visualize the distribution of the codewords and their proximity to each other. However, tools exist that attempt to reduce the dimensionality of data while preserving the relative distances between data points. The following code uses a tool called Isomap (a non-linear dimension-reduction technique, unlike PCA) to plot the data in 2D to give some intuition of the distribution of the codewords. How well grouped are the codewords for each speaker?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mfcclab.plot_codebook_2D(codeBook, labels, [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"s6\", \"s7\", \"s8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "n_neighbors = 5 # we use nearest neighbour\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
    "knn.fit(codeBook, labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"test/s2.wav\" #try load one testing wave file\n",
    "fs, data = wavfile.read(filename)\n",
    "\n",
    "mfcc = calcMFCC(fs, data, 20)\n",
    "\n",
    "mfcc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now predict the label of each frame you just computed. There will be some mislabeled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = np.round(knn.predict(mfcc[:,1:numKeptCoeff]))\n",
    "print l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict # this is used to find the most occured label\n",
    "d = defaultdict(int)\n",
    "for i in l:\n",
    "    d[i] += 1\n",
    "result = max(d.iteritems(), key=lambda x: x[1])\n",
    "print \"Matching speaker: %d\" % result[0] #the first result is the label, the second is the number of occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Extra Credit\n",
    "\n",
    "The current algorithm will predict a speaker regardless if it is in the database. Propose and implement a modification that will be able to do so. Record a short clip of your own voice and briefly describe and comment on your results and observations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
