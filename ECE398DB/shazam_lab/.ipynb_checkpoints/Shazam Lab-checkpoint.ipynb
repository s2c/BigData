{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, a [Shazam](http://bit.ly/1fvTDIQ)-similar system is implemented. An acoustic fingerprint of a song is generated by hashing the frequency and time information in the spectrogram. A fingerprint of a snippet is later generated and searched through the database of hashes. A high number of matches is found if the snippet comes from the corresponding song. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shazam: Audio Recognition System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we set the pylab inline option and import necessary modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy.ndimage.filters import maximum_filter # find the peaks in the spectrogram\n",
    "from scipy.ndimage.morphology import (generate_binary_structure,iterate_structure)\n",
    "from pydub import AudioSegment #read wav file.\n",
    "import hashlib # to generate hash from the frequency and time information\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function called readAudioFile to read the **wav** file. *limit* is the number of seconds from the beginning of the song we would like to read. If *limit* is None, readAudioFile returns the data for the whole song. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readAudioFile(filename, limit= None):\n",
    "    audiofile = AudioSegment.from_wav(filename)\n",
    "    if limit:\n",
    "        audiofile = audiofile[:limit * 1000]\n",
    "    data = np.fromstring(audiofile._data, np.int16)\n",
    "    channels = []\n",
    "    for chn in xrange(audiofile.channels):\n",
    "        channels.append(data[chn::audiofile.channels])        \n",
    "    return channels, audiofile.frame_rate, audiofile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the Shazam [paper](http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf). We will refer to figures in this paper. \n",
    "Next we define two functions:\n",
    "\n",
    "1. **get_2D_peaks**: Find the peaks in the spectrogram. Refer to figure 1B of the paper. The set of peaks of the spectrogram is also called the constellation map. \n",
    "2. **generate_hashes**: Generate the hashes from the constellation map. See figure 1C and 1D. The struture of the hashes is [f1:f2:delta_t]:t1. Then we call the hashlib function sha1 to generate a single number for our hash. For this demo, I keep the first 25 values of the hash. \n",
    "\n",
    "***Question***: does the accuracy of the algorithm increase or decrease if you increase the number of hashes to keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_2D_peaks(arr2D, plot,amp_min = 10):\n",
    "    struct   = generate_binary_structure(2,1)\n",
    "    neighborhood  = iterate_structure(struct, 20)\n",
    "    detected_peaks = maximum_filter(arr2D, footprint  = neighborhood) == arr2D\n",
    "    amps = arr2D[detected_peaks]\n",
    "    j,i  = np.where(detected_peaks) \n",
    "    amps = amps.flatten()\n",
    "    peaks = zip(j,i, amps)\n",
    "    peaks_filtered = [x for x in peaks if x[2]>amp_min]\n",
    "    frequency_idx = [x[1] for x in peaks_filtered]\n",
    "    time_idx = [x[0] for x in peaks_filtered]\n",
    "    if plot: \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(arr2D)\n",
    "        ax.scatter(frequency_idx, time_idx)\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title(\"Spectrogram\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.axis('tight')\n",
    "    return zip(frequency_idx, time_idx)\n",
    "def generate_hashes(peaks, fan_value = 10, len_hash = 25):\n",
    "    fingerprinted = set()  \n",
    "    for i in range(len(peaks)):\n",
    "        for j in range(fan_value):\n",
    "            if (i + j) < len(peaks) and not (i, i + j) in fingerprinted:\n",
    "                freq1 = peaks[i][1]\n",
    "                freq2 = peaks[i + j][1]\n",
    "                t1 = peaks[i][0]\n",
    "                t2 = peaks[i + j][0]\n",
    "                t_delta = t2 - t1\n",
    "                if t_delta >= 0:\n",
    "                    h = hashlib.sha1(\"%s|%s|%s\" % (str(freq1), str(freq2), str(t_delta))) \n",
    "                    yield (h.hexdigest()[0:len_hash], t1)\n",
    "                fingerprinted.add((i, i + j))             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function **fingerpint** returns the hashes of an audio file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fingerprint(samples, Fs, wsize, wratio, fan_value, amp_min, plot):\n",
    "    arr2D = mlab.specgram(samples, NFFT = wsize, Fs = Fs, window = mlab.window_hanning, noverlap = int(wsize * wratio))[0];\n",
    "    arr2D = 10 * np.log10(arr2D+0.001)\n",
    "    arr2D[arr2D == -np.inf] = 0\n",
    "    local_maxima = get_2D_peaks(arr2D, plot, amp_min = amp_min)\n",
    "    return generate_hashes(local_maxima, fan_value = fan_value, len_hash = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract and save the fingerprints. First we read the audio file from the wav folder. You can input your own wav file there. Define sampling rate, window size, overlap ratio between consecutive frames. [pydub](https://github.com/jiaaro/pydub/) can read mp3 file, but you need to configure ffmpeg. You might do that on your own computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "channels, framerate,data = readAudioFile('data/JonBoden.wav')\n",
    "Fs = 44100 # sampling freq\n",
    "wsize  = 4096 # window size\n",
    "wratio = 0.5 # overlap ratio\n",
    "fan_value = 10 #number of target points in the frequency domain to pair with the anchor point\n",
    "amp_min = 25 # minimum amplitude of the peak.\n",
    "plot = False\n",
    "for samples in channels:\n",
    "    hashes = fingerprint(samples, Fs, wsize, wratio, fan_value, amp_min, plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easy visualization and explanation, we will convert our hases into a dictionary of the form: **dict[hash] = t1**. Dictionaries consist of pairs of **keys** and their corresponding **values**. We use the hashes as keys and the time of the hash as value. Since different segments from the audio file can produce the same hash, one key might correspond to multiple values (multiple starting times t1). We use the *defaultdict* data structure to accomodate multiple-valued-single-key entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def hashesToDict(hashesExcerpt):\n",
    "    dataExcerpt  = []\n",
    "    for i in hashesExcerpt:\n",
    "        dataExcerpt.append(i)\n",
    "    dictExcerpt = defaultdict(list)\n",
    "    for hashString, t1 in dataExcerpt:\n",
    "        dictExcerpt[hashString].append(t1)\n",
    "    return dictExcerpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the hashes to a dictionary called **dictOriginal**. **dictOriginal** contains all the hashes of the *'JonBoden.wav'* song. The key is the hash and the value is the time. See figure 1D in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictOriginal = hashesToDict(hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement searching, we could implement [mySQL](http://mysql-python.sourceforge.net/) databases. This provides a systematic approach to store, retrieve and search for hashes. mySQL requires a mySQL server, which adds a level of complexity to this project. For now, we only save the fingperprints into a file and later retrieve them if we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "fileHandle  = open(\"JonBoden.fp\",\"wb\") #wb: write binary\n",
    "pickle.dump(dictOriginal, fileHandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the fingerprinted file back and store it as **dictOriginal**. You can fingerprint as many files as you want, then read all of them here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileHandle = open(\"JonBoden.fp\",\"rb\")\n",
    "dictOriginal = pickle.load(fileHandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**. Write a function called readAudioFileExcerpt with three inputs. See the readAudioFile function above for guidance. The first input is the filename. The other two inputs are lowLimit and highLimit, which is used to read the audio file from lowLimit to highLimt seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readAudioFileExcerpt(filename, lowLimit, highLimit):\n",
    "     # write your code here\n",
    "     return channels, audiofile.frame_rate, audiofile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 2**. Use your function to read the excerpt from the JonBoden.wav and generate the hashes from this snippet. Name the hahses **hashesExcerpt1**. Also, we've set the plot parameter to True to visualize the peaks of the spectrogram. What parameters in the **fingerprint** function affect the number of peaks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write your code here.\n",
    "# read in the audio snippit from 40s to 50s\n",
    "channelsExcerpt1, framerate, data = readAudioFileExcerpt('data/JonBoden.wav',40,50)\n",
    "plot = True # but now we want to visualize the peak (pass this as a parameter to get_2D_peaks)\n",
    "# call the fingerprint function to generate the hashes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the **hashesExcerpt1** to a dictionary. We can call the **hashesToDict** function defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictExcerpt1 = hashesToDict(hashesExcerpt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 3**. Write a function to match the fingerprint of the excerpt to the original song. Return count and delta_k. Count is the number of matching hashes. delta_k is a list of offsets of the database song time and the snippet song time. See section 2.3 of the paper. Read also [this document](http://www.tutorialspoint.com/python/python_dictionary.htm) on how to manipulate the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def match_fingerprint(dictExcerpt,dictOriginal):\n",
    "    count  = 0\n",
    "    delta_k = []\n",
    "    # write your code here\n",
    "    return count, delta_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how many matches are found and plot the histogram of the offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dictExcerpt1 = hashesToDict(hashesExcerpt1)\n",
    "count1, delta_k1 = match_fingerprint(dictExcerpt1, dictOriginal)\n",
    "if delta_k1:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    numBins = 10\n",
    "    ax.hist(delta_k1,numBins,color='green',alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**. Now load the second file, count the number of matches, and plot the histogram of the offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "channelsExcerpt2, framerate, data = readAudioFileExcerpt('data/BreakingtheHabit.wav',40,50)\n",
    "\n",
    "#write code to generate hashesExcerpt2\n",
    "\n",
    "dictExcerpt2 = hashesToDict(hashesExcerpt2)\n",
    "count2, delta_k2 = match_fingerprint(dictExcerpt2, dictOriginal)\n",
    "if delta_k2:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    numBins = 10\n",
    "    ax.hist(delta_k2,numBins,color='green',alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**. Based on the number of matches (consider how many possible matches there are) and histogram of offsets, justify that your code produces a desirable result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.** Now to test the system. Create your own dataset (of at least 100 songs) to test the accuracy of the Shazam system. You will need to generate a dictionary for each song. Use 10% of the songs in the dataset to test for recognition accuracy. Change the length of the hash (len_hash) (try several different values from 10 to 25) and amplitude threshold (amp_min) (try several different values from 5 to 10) and compare the accuracy, speed, and size of the dictionary. What do you think is the optimal trade-off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Credit.** There is one component not yet implemented, which is reading the audio snippet from the microphone. This is not mandatory, but you can try it yourself if you have time. It is suggested to use [pyaudio](http://people.csail.mit.edu/hubert/pyaudio/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
