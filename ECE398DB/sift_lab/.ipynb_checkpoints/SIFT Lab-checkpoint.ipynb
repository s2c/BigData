{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will investigate two highly useful features for image analysis: color histogram and SIFT. Both of these features are widely used in computer vision and image analysis problems of all kinds, such as face recognition, similar image search, image classification, and object detection.\n",
    "\n",
    "## Notes on OpenCV\n",
    "\n",
    "Due to certain legal restrictions, current updated packages in OpenCV no longer have SIFT builtin. Depending on the OS you are using there might be different instructions on how to build and install OpenCV with Python on SIFT. We will be using OpenCV 3.1.0 for this lab. Older versions should work as well, but you might have to change some lines of code. Depending on your comfort level, you may choose either pathway.\n",
    "\n",
    "If you chose to build OpenCV from source, this will take a while. After the build process completes, a `cv2.so` will be produced in the lib folder. Copy this file to your notebook directory, this will allow you to do `import cv2`. **DO NOT SUBMIT THIS FILE**\n",
    "\n",
    "Make sure to at least get OpenCV and SIFT working before you leave the lab today.\n",
    "\n",
    "Documentation of OpenCV can be found [here](http://docs.opencv.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *A note about submission*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, submit this notebook with all required code contained therin. If explanations or observations are asked for, create a text box in the notebook in the section of the Exercise and put your comments there.\n",
    "\n",
    "Since Exercise 2 requires the submission of a folder of the similar images found by your code, submit this notebook and that folder in a zip file together. Name the file \"yourNetID_lab12.zip.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For image analysis, much of the information of an image can be conveyed merely by its histogram, although other, more sofisticated features, such as SIFT, are often used and needed in big data problems. Here, we'll investigate how useful the histogram can be. For this section, you will need to download the Caltech-101 dataset (http://www.vision.caltech.edu/Image_Datasets/Caltech101/). This dataset contains 101 different object categories, each with between 40 to 800 images of size 300 x 200, roughly. (This dataset pales in comparison to other modern, \"big data\" image datasets, but it's the best that your computer can likely handle easily.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named cv2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8675bff9eb85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmisc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named cv2"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll look at three test-case images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beach = misc.imread('beach.png')\n",
    "desert1 = misc.imread('desert1.jpg')\n",
    "desert2 = misc.imread('desert2.jpg')\n",
    "\n",
    "plt.imshow(beach)\n",
    "plt.figure()\n",
    "plt.imshow(desert1)\n",
    "plt.figure()\n",
    "plt.imshow(desert2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the following two functions to generate and compare histograms. They are built upon OpenCV's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compares to histograms using the histogram intersection\n",
    "\n",
    "def compHist(h1, h2):\n",
    "    # h1, h2 - input histograms to compare, which should be of dimension (3,N) for color images and (1,N) for\n",
    "    #              grayscale images. If the two histograms are not the same dimensions, the output is a large\n",
    "    #              value to enforce their disimilarity\n",
    "    if h1.shape[0] != h2.shape[0]:\n",
    "        return 100\n",
    "    else:\n",
    "        x1 = 1 - cv2.compareHist(h1[0,:].astype('float32'), h2[0,:].astype('float32'), cv2.HISTCMP_INTERSECT)\n",
    "        x2 = 1 - cv2.compareHist(h1[1,:].astype('float32'), h2[1,:].astype('float32'), cv2.HISTCMP_INTERSECT)\n",
    "        x3 = 1 - cv2.compareHist(h1[2,:].astype('float32'), h2[2,:].astype('float32'), cv2.HISTCMP_INTERSECT)\n",
    "        comp = np.sqrt(x1**2 + x2**2 + x3**2)\n",
    "        return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generates a histogram from an input image\n",
    "\n",
    "def generateHist(img, n_bins):\n",
    "    # img - the input image\n",
    "    # n_bins - the number of bins of the output histogram\n",
    "    # returns: h - the histogram\n",
    "    k = len(img.shape)\n",
    "    if k == 2:\n",
    "        h = np.zeros((1,n_bins),dtype='float32')\n",
    "        M,N = img.shape\n",
    "        h[0,:][:, np.newaxis] = cv2.calcHist([img], [0], None, [n_bins], [0,256])\n",
    "        h = h/(M*N)\n",
    "    else:\n",
    "        h = np.zeros((3,n_bins),dtype='float32')\n",
    "        M,N,D = img.shape\n",
    "        h[0,:][:, np.newaxis] = cv2.calcHist([img], [0], None, [n_bins], [0,256])\n",
    "        h[1,:][:, np.newaxis] = cv2.calcHist([img], [1], None, [n_bins], [0,256])\n",
    "        h[2,:][:, np.newaxis] = cv2.calcHist([img], [2], None, [n_bins], [0,256])\n",
    "        h = h/(M*N)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use the above functions to generate histograms for the three test-cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q = 100\n",
    "\n",
    "hist_desert1 = generateHist(desert1, Q)\n",
    "hist_desert2 = generateHist(desert2, Q)\n",
    "hist_beach = generateHist(beach, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)\n",
    "f.set_size_inches((8,8))\n",
    "ax1.plot(hist_beach[0,:])\n",
    "ax1.plot(hist_beach[1,:])\n",
    "ax1.plot(hist_beach[2,:])\n",
    "ax1.set_title('Histograms of Beach and Desert Scenes')\n",
    "\n",
    "ax2.plot(hist_desert1[0,:])\n",
    "ax2.plot(hist_desert1[1,:])\n",
    "ax2.plot(hist_desert1[2,:])\n",
    "\n",
    "ax3.plot(hist_desert2[0,:])\n",
    "ax3.plot(hist_desert2[1,:])\n",
    "ax3.plot(hist_desert2[2,:])\n",
    "ax3.set_xlabel('Color Intensity Bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the kernel interesction as a comparison metric, we compare how \"similar\" are the histograms. Which two images are the most similar? (Remember that this metric is a distance, so smaller values mean more \"similar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Beach vs. Desert1:\\t' + str(compHist(hist_beach, hist_desert1)) # beach vs. desert1\n",
    "print 'Desert2 vs. Desert1:\\t' + str(compHist(hist_desert2, hist_desert1)) # desert2 vs. desert1\n",
    "print 'Beach vs. Desert2:\\t' + str(compHist(hist_beach, hist_desert2)) # beach vs. desert2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your own implementation of the histogram. It should take in an image and a parameter called \"n_bins,\" which is the number of bins for the histogram. (In an image, pixel values range from 0 - 255, but the histogram could have fewer than 255 bins. If, for instance, n_bins = 100, then each bin of the histogram would store between 2-3 different pixel intensity values.) For grayscale images, the output will be a 1D vector of dimension (1, n_bins). For color images, compute a 1D histogram of each channel with n_bins and form the three histograms into a 2D matrix of dimension (3, n_bins). The histogram should also be normalized by the total number of pixels, so that along each row it sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def histogram(image, n_bins):\n",
    "    if len(image.shape) != 3:       # grayscale image\n",
    "\n",
    "    else:                           # color image\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similar Image Search by Histogram Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the histogram as a feature for our image, let's see how well it works when searching for similar images in a large dataset. You will need to import the lab5.py file, as done below, to use a function to load the Caltech-101 dataset. You should have downloaded from [here](http://www.vision.caltech.edu/Image_Datasets/Caltech101/) and unzipped this dataset to the same directory as your notebook. It should be in a folder called '101_ObjectCategories.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lab5\n",
    "\n",
    "dataset_dir = '101_ObjectCategories/'\n",
    "\n",
    "n_images, classes, image_names = lab5.load_image_dataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates histograms for all of the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hists = []\n",
    "\n",
    "n_bins = 100\n",
    "\n",
    "for name in image_names:\n",
    "    temp_image = misc.imread(name)\n",
    "    temp_hist = generateHist(temp_image, n_bins)\n",
    "    hists.append(temp_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the test query images that we will use to search for similar images. Notice the diversity in their histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare a test histogram to the database\n",
    "test_ind = [450, 2423, 5211, 8134]\n",
    "n_bins = 100\n",
    "\n",
    "for i in test_ind:\n",
    "    test = misc.imread(image_names[i])\n",
    "    plt.figure()\n",
    "    plt.imshow(test)\n",
    "\n",
    "    test_hist = generateHist(test, n_bins)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(test_hist[0,:], color='red')\n",
    "    plt.plot(test_hist[1,:], color='green')\n",
    "    plt.plot(test_hist[2,:], color='blue')\n",
    "    plt.title('Histogram for Image ' + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below iterates through the above test query images and searches for their 10 most similar images. Fill in the missing elements to generate the histograms and compare them using your own histogram implementation from Exercise 1. The number of bins for the histogram should be n_bins = 100.\n",
    "\n",
    "The images will be stored in a directory called \"closest\" in the same location as your notebook. They will have suffixes denoting which of the four images they match and their order in terms of similarity. (Note that the first image should be the query image itself, since it is still in the dataset and is more similar to itself than any other image.) Submit this folder with your notebook for the lab.\n",
    "\n",
    "Do the results look reasonable? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "closest_dir = 'closest/'\n",
    "if not os.path.exists(closest_dir):\n",
    "    os.makedirs(closest_dir)\n",
    "\n",
    "n_bins = 100\n",
    "K = 10\n",
    "    \n",
    "for ind in test_ind:\n",
    "    print 'Finding closest ' + str(K) + ' images to ' + image_names[ind]\n",
    "    test = misc.imread(image_names[ind])\n",
    "    \n",
    "    test_hist = [] #edit this line to make things right\n",
    "\n",
    "    D = np.zeros(n_images)\n",
    "    for i in range(n_images):\n",
    "        D[i] =  0 #edit this line to make things right \n",
    "    idx = np.argsort(D)\n",
    "    \n",
    "    closest = []\n",
    "    for i in range(K):\n",
    "        closest.append(image_names[idx[i]])\n",
    "        img = misc.imread(closest[i])\n",
    "        misc.imsave(closest_dir + 'closest_' + str(ind) + '_' + str(i) + '.jpg', img)\n",
    "        print '\\tSaving closest image ' + str(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Invariant Feature Transform (SIFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use OpenCV's built-in function to perform SIFT. In this part, we will find and match the keypoints of two images. One application is recognizing the traffic sign using the camera. You can watch a demo [here](http://bit.ly/1l51Tra). Let's first load the image and visualize the keypoints.\n",
    "\n",
    "Documentation can be found [here](http://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html#gsc.tab=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "boxImage1 = cv2.imread('box.png')\n",
    "grayImage1= cv2.cvtColor(boxImage1,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#sift = cv2.SIFT() #use this for versions 3.0.0 and below\n",
    "sift = cv2.xfeatures2d.SIFT_create() #Use this for versions >= 3.1.0\n",
    "kp = sift.detect(grayImage1,None)\n",
    "\n",
    "img = boxImage1.copy()\n",
    "cv2.drawKeypoints(grayImage1,kp, img)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this for the second image which we would like to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxImage2 = cv2.imread('box_in_scene.png')\n",
    "grayImage2 = cv2.cvtColor(boxImage2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create() \n",
    "kp = sift.detect(grayImage2,None)\n",
    "\n",
    "img=boxImage2.copy()\n",
    "cv2.drawKeypoints(grayImage2,kp, img)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching the Keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for matching keypoints, we will use the Brute Force Matcher, which tries to evaluate every possible match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(grayImage1,None)\n",
    "kp2, des2 = sift.detectAndCompute(grayImage2,None)\n",
    "bfmatcher = cv2.BFMatcher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you are provided with some functions to help you compute matches and display them. Read through them and make sure you understand what's going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_and_draw(kp1, kp2, des1, des2, grayImage1, grayImage2):\n",
    "    raw_matches = bfmatcher.knnMatch(np.asarray(des1, np.float32), np.asarray(des2, np.float32), k = 2) #2\n",
    "    p1, p2, kp_pairs = filter_matches(kp1, kp2, raw_matches)\n",
    "    if len(p1) >= 4:\n",
    "        H, status = cv2.findHomography(p1, p2, cv2.RANSAC, 5.0)\n",
    "        #print '%d / %d  inliers/matched' % (np.sum(status), len(status))\n",
    "    else:\n",
    "        H, status = None, None\n",
    "        #print '%d matches found, not enough for homography estimation' % len(p1)\n",
    "    vis = explore_match(grayImage1, grayImage2, kp_pairs, status, H)\n",
    "    return vis\n",
    "def filter_matches(kp1, kp2, matches, ratio = 0.75):\n",
    "    mkp1, mkp2 = [], []\n",
    "    for m in matches:\n",
    "        if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "            m = m[0]\n",
    "            mkp1.append( kp1[m.queryIdx] )\n",
    "            mkp2.append( kp2[m.trainIdx] )\n",
    "    p1 = np.float32([kp.pt for kp in mkp1])\n",
    "    p2 = np.float32([kp.pt for kp in mkp2])\n",
    "    kp_pairs = zip(mkp1, mkp2)\n",
    "    return p1, p2, kp_pairs\n",
    "\n",
    "def explore_match(img1, img2, kp_pairs, status = None, H = None):\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "    vis = np.zeros((max(h1, h2), w1+w2), np.uint8)\n",
    "    vis[:h1, :w1] = img1\n",
    "    vis[:h2, w1:w1+w2] = img2\n",
    "    vis = cv2.cvtColor(vis, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    if H is not None:\n",
    "        corners = np.float32([[0, 0], [w1, 0], [w1, h1], [0, h1]])\n",
    "        corners = np.int32( cv2.perspectiveTransform(corners.reshape(1, -1, 2), H).reshape(-1, 2) + (w1, 0) )\n",
    "        cv2.polylines(vis, [corners], True, (255, 255, 255))\n",
    "\n",
    "    if status is None:\n",
    "        status = np.ones(len(kp_pairs), np.bool_)\n",
    "    p1 = np.int32([kpp[0].pt for kpp in kp_pairs])\n",
    "    p2 = np.int32([kpp[1].pt for kpp in kp_pairs]) + (w1, 0)\n",
    "\n",
    "    green = (0, 255, 0)\n",
    "    red = (0, 0, 255)\n",
    "    white = (255, 255, 255)\n",
    "    kp_color = (51, 103, 236)\n",
    "    for (x1, y1), (x2, y2), inlier in zip(p1, p2, status):\n",
    "        if inlier:\n",
    "            col = green\n",
    "            cv2.circle(vis, (x1, y1), 2, col, -1)\n",
    "            cv2.circle(vis, (x2, y2), 2, col, -1)\n",
    "        else:\n",
    "            col = red\n",
    "            r = 2\n",
    "            thickness = 3\n",
    "            cv2.line(vis, (x1-r, y1-r), (x1+r, y1+r), col, thickness)\n",
    "            cv2.line(vis, (x1-r, y1+r), (x1+r, y1-r), col, thickness)\n",
    "            cv2.line(vis, (x2-r, y2-r), (x2+r, y2+r), col, thickness)\n",
    "            cv2.line(vis, (x2-r, y2+r), (x2+r, y2-r), col, thickness)\n",
    "    vis0 = vis.copy()\n",
    "    for (x1, y1), (x2, y2), inlier in zip(p1, p2, status):\n",
    "        if inlier:\n",
    "            cv2.line(vis, (x1, y1), (x2, y2), green)\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The match_and_draw function takes care of finding the matching keypoints and drawing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairedImage = match_and_draw(kp1, kp2, des1, des2, grayImage1, grayImage2)\n",
    "plt.imshow(pairedImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the information reduction factor when the image is represented by these SIFT features? Write the Python code to calculate the information reduction factor.\n",
    "2. In the matching code above, BFMatcher was used to find matching pairs using the KNN algorithm. Suggest an improvement that might be able to decrease the run time of this step.\n",
    "3. Extra Credit: Reimplement the matching function to use your improvement suggested above. You may use SKLearn or OpenCV's built-ins for this.\n",
    "4. Choose any 3 images each from 2 different classes in the Caltech 101 dataset. Compute SIFT features and try to match them. Some images will match well while others will not. Briefly explain why this is so. Support your answer with some code.\n",
    "5. Extra Credit: RANSAC was used to filter out false matches. Bonus points for those who could give me an explanation of what is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code to compute and match SIFT features from Caltech 101 here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
