{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: SVM's, Kernel Tricks, Model Assessment and Selection [Classification, Part 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Varun Somani netid: vsomani2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due: February 11, 2016 12:00 AM [This is when Wednesday transitions to Thursday]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Will Need to Know For This Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kernel Tricks\n",
    "* Support Vector Machines\n",
    "* Model Assessment and Selection with Cross-Validation (especially don't train on your test data!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble (don't change this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rock-\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from numpy import genfromtxt\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Cross-validation (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which implements $5$-fold cross-validation to estimate the error of a classifier with cross-validation with the 0,1-loss for k-Nearest Neighbors (kNN). \n",
    "\n",
    "You will be given as input:\n",
    "* A (N,d) numpy.ndarray of training data, trainData (with N divisible by 5)\n",
    "* A length $N$ numpy.ndarray of training labels, trainLabels\n",
    "* A number $k$, for which cross-validated error estimates will be outputted for $1,\\ldots,k$\n",
    "\n",
    "Your output will be a vector (represented as a numpy.ndarray) err, such that err[i] is the cross-validated estimate of using i neighbors (err will be of length $k+1$; the zero-th component of the vector will be meaningless). \n",
    "\n",
    "So that this problem is easier to grade, take your folds to be 0:N/5, N/5:2N/5, ..., 4N/5:N for cross-validation (In general, the folds should be randomly divided).\n",
    "\n",
    "Use scikit-learn's sklearn.neighbors.KNeighborsClassifier to perform the training and classification for the kNN models involved. Do not use any other features of scikit-learn, such as things from sklearn.cross_validation. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifierError(truelabels,estimatedlabels):\n",
    "    return float(np.sum(truelabels!=estimatedlabels))/truelabels.size\n",
    "def crossValidationkNN(trainData,trainLabels,k):\n",
    "    #Put your code here\n",
    "    err=np.zeros(k+1)\n",
    "    err[0]=100 # Set err[0] to some random meaningless value\n",
    "    length = trainData.shape[0]\n",
    "    for i in xrange(1,k+1):   \n",
    "        classifier = neighbors.KNeighborsClassifier(i)\n",
    "        for n in xrange(0,5):\n",
    "           valset =  trainData[(n*length)/5:((n+1)*length)/5,:] # get the right validation set training data\n",
    "           vallabels = trainLabels[(n*length)/5:((n+1)*length)/5] # get the right validation set testing data\n",
    "           trainset = np.delete(trainData,np.s_[(n*length)/5:((n+1)*length)/5],axis = 0)\n",
    "           trainla  = np.delete(trainLabels,np.s_[(n*length)/5:((n+1)*length)/5],axis = 0)\n",
    "           fits = classifier.fit(trainset,trainla)\n",
    "           result = fits.predict(valset)\n",
    "           err[i] += classifierError(result,vallabels)\n",
    "        err[i] = err[i]/5   \n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load some data (acquired from <a href=\"http://www.cs.ubc.ca/~murphyk/\">K.P. Murphy</a>'s <a href=\"https://github.com/probml/pmtk3\"> PMTK tookit</a>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "problem1_tmp= genfromtxt('p1.csv', delimiter=',')\n",
    "#The training data which you will use is called \"traindata\"\n",
    "traindata=problem1_tmp[:200,:2]\n",
    "#The training labels are in \"labels\"\n",
    "trainlabels=problem1_tmp[:200,2]\n",
    "\n",
    "#The test data which you will use is called \"testdata\" with labels \"testlabels\"\n",
    "testdata=problem1_tmp[200:,:2]\n",
    "testlabels=problem1_tmp[200:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cross-validation error versus number of neighbors for $1,\\ldots,50$ neighbors. <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xabe6518>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRHj/n34N+mwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaa73f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = crossValidationkNN(traindata,trainlabels,50)\n",
    "x = np.linspace(1, 50, 50)\n",
    "#print(x.shape[0], y[])\n",
    "pylab.plot(x,y[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the number of neighbors which minimizes the cross-validation error. What is the cross-validation error for this number of neighbors? <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.27000000000000002, ' is the lowest error at k = ', 16)\n"
     ]
    }
   ],
   "source": [
    "print (min(y) , \" is the lowest error at k = \" , np.argmin(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K = 16, error is 0.27 or 27% which is the lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a kNN model on the whole training data using the number of neighbors you found in the previous part of the question, and apply it to the test data. How does the test error compare to the cross-validation error you found in the last part of the problem? <b>(5 points)</b>\n",
    "\n",
    "For some ideas as to \"why\" you have this result, see section 7.10 of Elements of Statistical Learning, 2e, by Hastie et. al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.206\n"
     ]
    }
   ],
   "source": [
    " classifier = neighbors.KNeighborsClassifier(16)\n",
    " fits = classifier.fit(traindata,trainlabels)\n",
    " result = fits.predict(testdata)\n",
    " print(classifierError(result,testlabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test error is lower than the crossvalidation error, it is only 0.206 or about 21%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: SVM's, Model Selection & Assessment (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will be training a few SVM's with <a href=\"http://scikit-learn.org/stable/modules/svm.html#svm\">sklearn.svm</a> in order to perform model selection and assessment. \n",
    "\n",
    "You will be using sklearn.svm.SVC for non-linear kernels and sklearn.svm.LinearSVC for linear kernels. Your model selection  will be done with cross-validation via sklearn.cross_validation's cross_val_score. This returns the accuracy for each fold, i.e. the fraction of samples classified correctly. Thus, the cross-validation error is simply 1-mean(cross_val_score).\n",
    "\n",
    "All cross-validation should be 5-fold cross-validation.\n",
    "\n",
    "<b>Warning: This problem may require a lot of time to train the SVM's. Do not put this off until the last minute.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load some data (acquired from <a href=\"https://archive.ics.uci.edu/ml/datasets/Spambase\">here</a>). We will use scikit-learn's train test split function to split the data. The data is scaled for reasons outlined <a href=\"http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\">here</a>. In short, it helps avoid some numerical issues and avoids some problems with certain features which are typically large affecting the SVM optimization problem unfairly compared to features which are typically small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "spambase= genfromtxt('spambase.data', delimiter=',')\n",
    "\n",
    "spambase_features=spambase[:,:-1]\n",
    "spambase_labels=spambase[:,-1]\n",
    "\n",
    "#The training data is in data_train with labels label_train. The test data is in data_test with labels label_test.\n",
    "data_train, data_test, label_train, label_test = train_test_split(spambase_features,spambase_labels,test_size=0.2,random_state=372015)\n",
    "\n",
    "# Rescale the training data and scale the test data correspondingly\n",
    "scaler=MinMaxScaler(feature_range=(-1,1))\n",
    "data_train=scaler.fit_transform(data_train) #Note that the scaling is determined solely via the training data!\n",
    "data_test=scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a linear SVM with sklearn.svm.LinearSVC on the training data with the parameter $C$ varied from $2^{-5},2^{-4},\\ldots,2^{15}$. Which value of $C$ would you choose (and why), and what is its cross-validation error? The parameter $C$ controls how much points can be within the margin. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.075815217391304257, 'was the lowest error at C= ', 4)\n"
     ]
    }
   ],
   "source": [
    "#Put your code here\n",
    "begin = -5\n",
    "end =  15\n",
    "error = err=np.zeros(1+((end - begin)) )\n",
    "count = 0\n",
    "for i in xrange(begin,end + 1):\n",
    "    r = 2**i\n",
    "    clf = svm.LinearSVC(C=r)\n",
    "    scores=cross_validation.cross_val_score(\n",
    "    clf, data_train, label_train, cv=5)\n",
    "    error[count] = 1-mean(scores)\n",
    "    count = count + 1\n",
    "print(min(error), \"was the lowest error at C= \", 2**(-5 +(np.argmin(error))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of C I would choose would be C=4 as it had the lowest crossvalidation score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now experiment with kernels. Using sklearn.svm.SVC, calculate the cross-validation error of a SVM using a RBF with parameters $(C,\\gamma)$ varied over $C=2^{-5},2^{-4},\\ldots,2^{15}$ and $\\gamma=2^{-15},\\ldots,2^{3}$ [So, you will try about 400 parameter choices]. Out of these, which was the best? What was its cross-validation error? This procedure is known as a *grid search*.\n",
    "\n",
    "The parameter $C$ controls the amount points can be within the margin, while $\\gamma$ is a parameter in the RBF. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Parameters: \n",
      "('C = ', 16384, 'G = ', 0.015625)\n"
     ]
    }
   ],
   "source": [
    "#Put your code here\n",
    "#Put your code here\n",
    "begin_c = -5\n",
    "end_c =  15\n",
    "begin_g = -15\n",
    "end_g = 3\n",
    "error_rbf = err=np.zeros((1 +(end_g-begin_g),1+(end_c - begin_c)) )\n",
    "print(error_rbf.shape)\n",
    "g_count = 0\n",
    "for g in xrange(begin_g,end_g + 1):    \n",
    "    c_count = 0\n",
    "    for i in xrange(begin_c,end_c + 1):\n",
    "        r = 2**i\n",
    "        l = 2**g\n",
    "        clf = svm.SVC(C=r,gamma = l)\n",
    "        scores=cross_validation.cross_val_score(\n",
    "        clf, data_train, label_train, cv=5)\n",
    "        error_rbf[g_count,c_count] = 1-mean(scores)\n",
    "        c_count = c_count + 1\n",
    "    g_count = g_count +1\n",
    "lowest_error = np.amin(error_rbf)\n",
    "t =np.unravel_index(error_rbf.argmin(), error_rbf.shape)\n",
    "c_loweest_error =2**(-5 +(t[1]))\n",
    "g_loweest_error =2**(-15 + t[0])\n",
    "print(\"Lowest Parameters: \")\n",
    "print( \"C = \",c_loweest_error,\"G = \",g_loweest_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058152173913\n"
     ]
    }
   ],
   "source": [
    "print (lowest_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest error parameters were C = 16384 = 2^14 , G = 0.015625 = 2^-6 with cross validation error 0.058152173913"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, estimate the cross-validation error of applying LDA to this data.\n",
    "\n",
    "Between the linear SVM, the SVM using RBF's from the previous part of the problem, and the LDA classifier, which would you choose to use? Why? Make sure to take into account error and computational considerations.<b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.108967391304\n"
     ]
    }
   ],
   "source": [
    "clf_lda = LDA()\n",
    "scores_lda=cross_validation.cross_val_score(\n",
    "clf_lda, data_train, label_train, cv=5)\n",
    "print (1 - mean(scores_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest error was from SVM using RBF, but it took about 30 minutes to run completely on a beefy computer. The LDA ran the quickest, but it had about 11% error, which is significantly higher than both SVMs. The linear SVM was a healthy balance between the accuracy from using an SVM and the speed of an LDA, taking about 3-4 minutes to run. As such, if reasonable accuracy was a concern, I would pick the linear SVM, but if we  require a speedy solution with mostly correct predictions, an LDA works perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the parameters chosen in the previous problem to train a classifier on the whole training set. Then, estimate the prediction error using the test set. What is your estimate of the prediction error? How does it compare to the cross-validation error?  <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0901194353963\n",
      "0.0684039087948\n"
     ]
    }
   ],
   "source": [
    "clf_svm_line = svm.LinearSVC(C=4)\n",
    "clf_svm_line.fit(data_train,label_train)\n",
    "predictions_line = clf_svm_line.predict(data_test)\n",
    "print (classifierError(predictions_line,label_test))\n",
    "\n",
    "clf_svm_rbf = svm.SVC(C=16384, gamma=0.015625)\n",
    "clf_svm_rbf.fit(data_train,label_train)\n",
    "predictions_rbf = clf_svm_rbf.predict(data_test)\n",
    "print (classifierError(predictions_rbf,label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error for the linear svm with the optimized parameters was 0.09, which was also slightly higher than the cross validation error\n",
    "Error for the rbf svm with the optimized parameters was 0.07, which was also slightly higher than the cross validation error\n",
    "These results were about what we expected, as genaralisation error is usually higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
