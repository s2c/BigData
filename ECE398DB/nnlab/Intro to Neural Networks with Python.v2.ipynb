{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2", 
   "language": "python", 
   "name": "python2"
  }, 
  "language_info": {
   "codemirror_mode": {
    "name": "ipython", 
    "version": 2
   }, 
   "file_extension": ".py", 
   "mimetype": "text/x-python", 
   "name": "python", 
   "nbconvert_exporter": "python", 
   "pygments_lexer": "ipython2", 
   "version": "2.7.10"
  }, 
  "name": ""
 }, 
 "nbformat": 2, 
 "nbformat_minor": 0, 
 "orig_nbformat": 4, 
 "orig_nbformat_minor": 0, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "import numpy as np", 
      "import matplotlib.pyplot as plt", 
      "import theano.tensor as T", 
      "from theano import function", 
      "from theano import pp", 
      "from random import random", 
      "from theano import shared", 
      "", 
      "%matplotlib inline"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "# Brief intro to Theano", 
      "", 
      "Theano is a Python library that helps you with the computation of multi-dimensional arrays efficiently. It integrates well with NumPy and it's one of the work horses of modern machine learning. A complete coverage of Theano and deep learning is well beyond the scope of this course. The concepts here should translate naturally, with some minor differences in terminology, to other packages, like the recently release TensorFlow from Google, and Torch from Facebook. This lab is designed to serve as a gentle introduction of the mathematics behind deep learning and Theano, the majority of the code is already provided and you should not need more than a line or 2 for each of the exercises."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "## Defining Functions", 
      ""
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "A simple function"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "a = T.dscalar('a')", 
      "b = T.dscalar('b')", 
      "c = a*b", 
      "f = function([a,b],c)"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "f(1.5,3)"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "Returning multiple variables"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "a = T.dscalar('a')", 
      "f = function([a],[a**2, a**3])", 
      "f(3)"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "Computing Gradients"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "x = T.dscalar('x')", 
      "y = x**3", 
      "qy = T.grad(y,x)", 
      "f = function([x],qy)", 
      "f(4)"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "### Exercise:", 
      "", 
      "1. Write a Theano function that defines a 2D Gaussian (with mean zero and sigma = 1)", 
      "2. Use Theano to compute it's derivative", 
      "3. Evaluate these functions at several points and plot them to see if they agree with what you would expect on paper"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "A 2D Gaussian is define as follows:", 
      "$$ y = A \\exp{\\left(-\\left(\\frac{(x_1-\\mu_{x_1})^2}{2\\sigma^2_{x_1}} + \\frac{(x_2-\\mu_{x_2})^2}{2\\sigma^2_{x_2}}\\right)\\right)} $$"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "## Shared Variables", 
      "", 
      "Shared variables are usually used to store \"constants\" in a function that can change. That is, they are constant when evaluating the function, but we can choose to update them if they don't work for us. For example, in the Gaussian function above, we might want to change $\\sigma$ or $\\mu$ after evaluating with a certain set of $x_1, x_2$."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "x = T.iscalar('x')", 
      "sh = shared(0)", 
      "f = function([x], sh**2, updates=[(sh,sh+x)])"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "Run `f(1)` multiple times, explain briefly "
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "f(1)"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "# Modeling a Single Neuron", 
      "", 
      "", 
      "Recall from the lecture that our simplistic model of a neuron is simply a weighted sum of its inputs and a bias term with a function applied.", 
      "", 
      "The weighted sum of inputs can be computed as a dot product between the weights vector $w$ and the input vector $x$. ", 
      "", 
      "$$output = f( w^T x +b )$$", 
      "", 
      "In this example, $f$ is defined as:", 
      "", 
      "$$ f(x) = \\begin{cases} 0 & x\\lt 0 \\\\", 
      "1 & x\\geq 0 ", 
      "\\end{cases} $$"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "from theano.ifelse import ifelse", 
      "import numpy as np", 
      "", 
      "#Define variables:", 
      "x = T.vector('x')", 
      "w = T.vector('w')", 
      "b = T.scalar('b')", 
      "", 
      "#Define mathematical expression:", 
      "z = T.dot(x,w)+b", 
      "a = ifelse(T.lt(z,0),0,1)", 
      "", 
      "neuron = function([x,w,b],a)"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "## Feed forward pass", 
      "", 
      "The feed forward pass is the computation of a neuron output from an input vector. "
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "#Define inputs and weights", 
      "inputs = [", 
      "    [0, 0],", 
      "    [0, 1],", 
      "    [1, 0],", 
      "    [1, 1]", 
      "]", 
      "weights = [ 1, 1]", 
      "bias = -1.5", 
      "", 
      "#Iterate through all inputs and find outputs:", 
      "for i in range(len(inputs)):", 
      "    t = inputs[i]", 
      "    out = neuron(t,weights,bias)", 
      "    print 'The output for x1=%d | x2=%d is %d' % (t[0],t[1],out)"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "Notice that the neuron function takes in weights as it's arguments. Typically in a neural network, we will be updating the weights and biases when we train the network. It would be good if we could implement the weights as a shared variable.", 
      "", 
      "**Exercise** Reimplement `neuron()` using shared variables for weights and biases"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "#Define variables:", 
      "x = T.vector('x')", 
      "#Your code here", 
      "", 
      "#Define mathematical expression:", 
      "#Same as before", 
      "z = T.dot(x,w)+b", 
      "a = ifelse(T.lt(z,0),0,1)", 
      "", 
      "#Replace this line", 
      "neuron = function([x,w,b],a)  #the function should no longer have x, b", 
      "", 
      "#Define inputs and weights", 
      "inputs = [", 
      "    [0, 0],", 
      "    [0, 1],", 
      "    [1, 0],", 
      "    [1, 1]", 
      "]", 
      "", 
      "#Iterate through all inputs and find outputs:", 
      "for i in range(len(inputs)):", 
      "    t = inputs[i]", 
      "    out = neuron(t)", 
      "    print 'The output for x1=%d | x2=%d is %d' % (t[0],t[1],out)"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "The feedforward step is now completed. We are now ready to allow the neurons to learn the weights based on some measure if the output fits what we desire. This is done by using back propagation."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "## Back propagation", 
      "", 
      "Training a neural network involves finding out weights for the input values to the neurons. Here our neuron is the logistic function,", 
      "", 
      "$$ y = \\frac{1}{1+\\exp{-(w^Tx+b)}}$$"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "#Define variables:", 
      "x = T.matrix('x')", 
      "w = shared(np.array([random(),random()]))", 
      "b = shared(1.)", 
      "learning_rate = 0.01", 
      "", 
      "#Define mathematical expression:", 
      "z = T.dot(x,w)+b", 
      "a = 1/(1+T.exp(-z))"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "Now we need to define a measure of how different is the neuron's output from our desired output. We call this the cost."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "a_hat = T.vector('a_hat') #Actual output", 
      "cost = -(a_hat*T.log(a) + (1-a_hat)*T.log(1-a)).sum()"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "We now make use of Theano to do the differentiation for us"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "dw,db = T.grad(cost,[w,b])", 
      "", 
      "train = function(", 
      "    inputs = [x,a_hat],", 
      "    outputs = [a,cost],", 
      "    updates = [", 
      "        [w, w-learning_rate*dw],", 
      "        [b, b-learning_rate*db]", 
      "    ]", 
      ")"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "#Define inputs and weights", 
      "inputs = [", 
      "    [0, 0],", 
      "    [0, 1],", 
      "    [1, 0],", 
      "    [1, 1]", 
      "]", 
      "outputs = [0,0,0,1]", 
      "", 
      "#Iterate through all inputs and find outputs:", 
      "cost = []", 
      "for iteration in range(30000):", 
      "    pred, cost_iter = train(inputs, outputs)", 
      "    cost.append(cost_iter)", 
      "    ", 
      "#Print the outputs:", 
      "print 'The outputs of the NN are:'", 
      "for i in range(len(inputs)):", 
      "    print 'The output for x1=%d | x2=%d is %.2f' % (inputs[i][0],inputs[i][1],pred[i])", 
      "    ", 
      "#Plot the flow of cost:", 
      "print '\\nThe flow of cost during model run is as following:'", 
      "import matplotlib.pyplot as plt", 
      "", 
      "plt.plot(cost)"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "There is very little coding involed in this lab, make sure you understand everything that's going on here."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "# Modeling a two-layer network"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "x = T.matrix('x')", 
      "w1 = shared(np.array([random(),random()]))", 
      "w2 = shared(np.array([random(),random()]))", 
      "w3 = shared(np.array([random(),random()]))", 
      "b1 = shared(1.)", 
      "b2 = shared(1.)", 
      "learning_rate = 0.01"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "a1 = 1/(1+T.exp(-T.dot(x,w1)-b1))", 
      "a2 = 1/(1+T.exp(-T.dot(x,w2)-b1))", 
      "x2 = T.stack([a1,a2],axis=1)", 
      "a3 = 1/(1+T.exp(-T.dot(x2,w3)-b2))"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "a_hat = T.vector('a_hat') #Actual output", 
      "cost = -(a_hat*T.log(a3) + (1-a_hat)*T.log(1-a3)).sum()", 
      "dw1,dw2,dw3,db1,db2 = T.grad(cost,[w1,w2,w3,b1,b2])", 
      "", 
      "train = function(", 
      "    inputs = [x,a_hat],", 
      "    outputs = [a3,cost],", 
      "    updates = [", 
      "        [w1, w1-learning_rate*dw1],", 
      "        [w2, w2-learning_rate*dw2],", 
      "        [w3, w3-learning_rate*dw3],", 
      "        [b1, b1-learning_rate*db1],", 
      "        [b2, b2-learning_rate*db2]", 
      "    ]", 
      ")"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "inputs = [", 
      "    [0, 0],", 
      "    [0, 1],", 
      "    [1, 0],", 
      "    [1, 1]", 
      "]", 
      "outputs = [1,0,0,1]", 
      "", 
      "#Iterate through all inputs and find outputs:", 
      "cost = []", 
      "for iteration in range(30000):", 
      "    pred, cost_iter = train(inputs, outputs)", 
      "    cost.append(cost_iter)", 
      "    ", 
      "#Print the outputs:", 
      "print 'The outputs of the NN are:'", 
      "for i in range(len(inputs)):", 
      "    print 'The output for x1=%d | x2=%d is %.2f' % (inputs[i][0],inputs[i][1],pred[i])", 
      "    ", 
      "#Plot the flow of cost:", 
      "print '\\nThe flow of cost during model run is as following:'", 
      "", 
      "plt.plot(cost)"
     ], 
     "language": "python", 
     "metadata": {}, 
     "outputs": [], 
     "prompt_number": null
    }, 
    {
     "cell_type": "markdown", 
     "metadata": {}, 
     "source": [
      "### Exercise", 
      "", 
      "Try some other input values and desired output values. Are the neurons able to learn what you desire?"
     ]
    }
   ], 
   "metadata": {}
  }
 ]
}